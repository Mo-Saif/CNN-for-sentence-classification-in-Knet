{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Pkg; Pkg.add(\"CorpusLoaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, Random, Plots, IterTools\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten\n",
    "using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.06731994 0.05295623 … -0.21142985 0.013637338; -0.05344657 0.06545979 … -0.0087888 -0.07428761; … ; -0.0073346887 0.010894641 … -0.0040515745 0.015611163; -0.0051456536 -0.047072206 … -0.034157887 0.039655942], [\"</s>\", \"in\", \"for\", \"that\", \"is\", \"on\", \"##\", \"The\", \"with\", \"said\"  …  \"#-###-PA-PARKS\", \"Lackmeyer\", \"PERVEZ\", \"KUNDI\", \"Budhadeb\", \"Nautsch\", \"Antuane\", \"tricorne\", \"VISIONPAD\", \"RAFFAELE\"])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 30 # max sentence length\n",
    "# V = length(w2v.vocab) # vocab size\n",
    "B = 50 # batch size\n",
    "D = 100 # embedding size\n",
    "C = 6 # number of classes\n",
    "Ci = 1 # number of input channels\n",
    "Ks = [3,4,5] # kernel_sizes\n",
    "Co = 100 # output channels per kernel size\n",
    "dp = 0.5; # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(w2v.vocab));\n",
    "\n",
    "# function get_embedding(word)\n",
    "#     ind = get_word_index[word]\n",
    "#     emb = w2v.embeddings[:,ind]\n",
    "#     return emb\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw2i = Dict{String,Int}()\n",
    "unk = get_word_index[\"UNK\"]\n",
    "pad = get_word_index[\"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readdata (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readdata(trnf, tstf, S)\n",
    "    ixtrn = []\n",
    "    ixtst = []\n",
    "    \n",
    "    # read lines\n",
    "    trnlines = readlines(trnf)\n",
    "    \n",
    "    # split label from sentence \n",
    "    ytrn = [split(i)[1] for i in trnlines]\n",
    "    ytrn = [split(i, \":\")[1] for i in ytrn]\n",
    "    xtrn = [split(i)[2:end] for i in trnlines]\n",
    "    \n",
    "    # encode label\n",
    "    ytrn = [get!(yw2i, w, 1+length(yw2i)) for w in ytrn]\n",
    "    \n",
    "    \n",
    "    tstlines = readlines(tstf)\n",
    "    ytst = [split(i)[1] for i in tstlines]\n",
    "    ytst = [split(i, \":\")[1] for i in ytst]\n",
    "    ytst = [get(yw2i, w, 0) for w in ytst]\n",
    "    xtst = [split(i)[2:end] for i in tstlines]\n",
    "    \n",
    "    # encode sentences and pad or truncate\n",
    "    for (i, line) in enumerate(xtrn)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtrn, words)\n",
    "    end\n",
    "    \n",
    "    for (i, line) in enumerate(xtst)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtst, words)\n",
    "    end\n",
    "    \n",
    "    # concatenate to one big matrix\n",
    "    ixtrn, ixtst = hcat(ixtrn...), hcat(ixtst...)\n",
    "#     @show size(ixtst)\n",
    "    \n",
    "    # reduce the size of the \n",
    "    unq = [] # unique words in the training and test sets\n",
    "    push!(unq, unique(ixtrn)...)\n",
    "    push!(unq, unique(ixtst)...)\n",
    "    unq = unique(unq)\n",
    "    sort!(unq)\n",
    "#     @show size(unq)\n",
    "    trans = Dict(j => i for (i,j) in enumerate(unq)) \n",
    "    ixtrn = [get(trans, i, 0) for i in ixtrn]\n",
    "#     @show size(ixtrn)\n",
    "    ixtst = [get(trans, i, 0) for i in ixtst]\n",
    "    embeddings = KnetArray(w2v.embeddings[:,unq])\n",
    "    global V = size(embeddings, 2)\n",
    "    # create iterators\n",
    "    dtrn, dtst = minibatch(ixtrn, ytrn, B, shuffle=true), minibatch(ixtst, ytst, B, shuffle=true)\n",
    "    return dtrn, dtst, embeddings\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size(unq) = (8970,)\n",
      "size(ixtrn) = (30, 5452)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([988 423 … 423 423; 88 1788 … 4 1699; … ; 6647 6647 … 6647 6647; 6647 6647 … 6647 6647], [1 2 … 5 2], 50, 5452, false, 5403, 1:5452, true, (30, 5452), (5452,), Array{Int64,2}, Array{Int64,1}), Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([988 423 … 423 423; 313 787 … 4 4; … ; 6647 6647 … 6647 6647; 6647 6647 … 6647 6647], [5 6 … 2 1], 50, 500, false, 451, 1:500, true, (30, 500), (500,), Array{Int64,2}, Array{Int64,1}), K32(300,8970)[0.05295623⋯])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn, dtst, embedding = readdata(\"./TREC/train_5500.label.txt\", \"./TREC/TREC_10.label.txt\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed layer # may load pretrained embeddings instead\n",
    "struct Embed; w; end\n",
    "(l::Embed)(x) = reshape(l.w[:,x], (size(l.w,1),size(x,1),1,size(x,2))) # E,S,Cx,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Conv; w; b; f; p; end\n",
    "function (c::Conv)(x)\n",
    "    conved = conv4(c.w, dropout(x,c.p)) .+ c.b\n",
    "    return c.f.(pool(conved; window=(size(conved, 1), size(conved, 2))))\n",
    "end\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct cnntext\n",
    "    V # vocab size\n",
    "    D # embedding size\n",
    "    C # number of classes\n",
    "    Ci # one\n",
    "    Ks # kernel_sizes\n",
    "    Co # number of each of the sizes\n",
    "    dp # dropout probability\n",
    "    embed::Embed # embedding layer\n",
    "    fc::Dense \n",
    "    conv1::Conv\n",
    "    conv2::Conv\n",
    "    conv3::Conv\n",
    "end\n",
    "\n",
    "function (c::cnntext)(x)\n",
    "    x = c.embed(x)  # E, S, Cx (1), B\n",
    "    x1, x2, x3 = c.conv1(x), c.conv2(x), c.conv3(x) # \n",
    "    x = cat(x1,x2,x3, dims=1)\n",
    "    x = c.fc(x)\n",
    "end\n",
    "\n",
    "(c::cnntext)(x,y) = nll(c(x),y)\n",
    "\n",
    "(c::cnntext)(d::Knet.Data) = mean(c(x,y) for (x,y) in d)\n",
    "\n",
    "function cnntext(V, D, C, Ci, Ks, Co, dp)  ##############\n",
    "    cnntext(V, D, C, Ci, Ks, Co, dp, Embed(Param(embedding)), Dense(length(Ks)*Co,C,identity,pdrop=dp), Conv(D,Ks[1],Ci,Co), Conv(D,Ks[3],Ci,Co), Conv(D,Ks[3],Ci,Co))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext(8970, 100, 6, 1, [3, 4, 5], 100, 0.5, Embed(P(KnetArray{Float32,2}(300,8970))), Dense(P(KnetArray{Float32,2}(6,300)), P(KnetArray{Float32,1}(6)), identity, 0.5), Conv(P(KnetArray{Float32,4}(100,3,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnntext(V, D, C, Ci, Ks, Co, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×50 Array{Int64,2}:\n",
       "  423   423    67   988   423   423  …   423  3150   423  1935   423   423\n",
       "    9     4  7906   124    55     4     1895   831     4     4   184     4\n",
       " 2938  1895  1047  4426    10  7906     5198    88  4147  6529     1    10\n",
       " 2594  7906  7906   169  7906   500     2706  8699  5289  5607  7906  3732\n",
       " 7906  6647    80    25  5507  5214        4  7562  7906  2136    33  7906\n",
       "  666  6647   169  6350     5     1  …  1251  1359   765  3108    10    10\n",
       " 1380  6647  7900    20  7906  2856     2636  7906  7906  1383   101   435\n",
       "  502  6647   956  7906  7109  3563       10  6647  2178  5521  5396  5199\n",
       " 7906  6647  7906  6647  3160  7906     1892  6647  7906    65  2665  6163\n",
       " 6647  6647  6647  6647   978  6647      591  6647  6647   430  7906  7906\n",
       " 6647  6647  6647  6647     2  6647  …  7906  6647  6647     1  1118  6647\n",
       " 6647  6647  6647  6647  7906  6647     6647  6647  6647  7906  1080  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  7906  6647\n",
       "    ⋮                             ⋮  ⋱           ⋮                        \n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647  …  6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647  …  6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(dtrn)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×50 KnetArray{Float32,2}:\n",
       "  0.0251474    0.029747     0.0268792   …   0.0295141    0.0232194 \n",
       "  0.00715772   0.00108968   0.00335438      0.00522249   0.00180868\n",
       "  0.0681352    0.0621302    0.0648593       0.065296     0.0695555 \n",
       " -0.0168696   -0.0229652   -0.0131215      -0.0155905   -0.00671204\n",
       "  0.028964     0.0292695    0.0241421       0.0234454    0.0334205 \n",
       " -0.0276406   -0.0244257   -0.0299898   …  -0.026911    -0.0293199 "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(first(dtrn)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        r = ((model(dtrn), model(dtst), accuracy(model,dtrn), accuracy(model,dtst))\n",
    "             for x in takenth(progress(adam(model,ncycle(dtrn,nepochs))),length(dtrn)))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || return\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣███████████████████▉┫ [100.00%, 218/218, 09:09/09:09, 2.52s/i] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.38689047; 0.4562152; 0.6981651; 0.712]\n"
     ]
    }
   ],
   "source": [
    "cnn = trainresults(\"cnntextTREC.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float32,2}:\n",
       " 0.918583  0.38689 \n",
       " 1.0056    0.456215\n",
       " 0.698165  0.899266\n",
       " 0.712     0.86    "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This program has requested access to the data dependency word2vec 300d.\n",
    "# which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
    "\n",
    "# Pretrained Word2Vec Word emeddings\n",
    "# Website: https://code.google.com/archive/p/word2vec/\n",
    "# Author: Mikolov et al.\n",
    "# Year: 2013\n",
    "\n",
    "# Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "# Paper:\n",
    "#     Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "\n",
    "\n",
    "# Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/home/saif/.julia/datadeps/word2vec 300d\"?\n",
    "# [y/n]\n",
    "# stdin> y\n",
    "# const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l = length.(xtrn)\n",
    "\n",
    "# using StatsBase;\n",
    "# a = countmap(l);\n",
    "\n",
    "# b = hcat([[key, val] for (key, val) in a]...)';\n",
    "\n",
    "# sort(collect(a), by=x->x[2]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
