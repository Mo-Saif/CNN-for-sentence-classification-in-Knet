{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Pkg; Pkg.add(\"CorpusLoaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, Random, Plots, IterTools\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten\n",
    "# using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 40 # max sentence length\n",
    "B = 50 # batch size\n",
    "D = 100 # embedding size\n",
    "C = 6 # number of classes\n",
    "Ci = 1 # number of input channels\n",
    "Ks = [3,4,5] # kernel_sizes\n",
    "Co = 100 # output channels per kernel size\n",
    "dp = 0.5; # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i = Dict{String,Int}()\n",
    "yw2i = Dict{String,Int}()\n",
    "unk = get!(w2i, \"<unk>\", 1+length(w2i))\n",
    "pad = get!(w2i, \"<pad>\", 1+length(w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trnlines = readlines(\"./TREC/train_5500.label.txt\")\n",
    "# ytrn = [split(i)[1] for i in trnlines]\n",
    "# ytrn = [split(i, \":\")[1] for i in ytrn]\n",
    "# xtrn = [split(i)[2:end] for i in trnlines];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l = length.(xtrn)\n",
    "\n",
    "# using StatsBase;\n",
    "# a = countmap(l);\n",
    "\n",
    "# b = hcat([[key, val] for (key, val) in a]...)';\n",
    "\n",
    "# sort(collect(a), by=x->x[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tstlines = readlines(\"./TREC/TREC_10.label.txt\")\n",
    "# ytst = [split(i)[1] for i in tstlines]\n",
    "# ytst = [split(i, \":\")[1] for i in ytst]\n",
    "# xtst = [split(i)[2:end] for i in tstlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readdata (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readdata(trnf, tstf, S)\n",
    "    ixtrn = []\n",
    "    ixtst = []\n",
    "    \n",
    "    # read lines\n",
    "    trnlines = readlines(trnf)\n",
    "    \n",
    "    # split label from sentence \n",
    "    ytrn = [split(i)[1] for i in trnlines]\n",
    "    ytrn = [split(i, \":\")[1] for i in ytrn]\n",
    "    xtrn = [split(i)[2:end] for i in trnlines]\n",
    "    \n",
    "    # encode label\n",
    "    ytrn = [get!(yw2i, w, 1+length(yw2i)) for w in ytrn]\n",
    "    \n",
    "    \n",
    "    tstlines = readlines(tstf)\n",
    "    ytst = [split(i)[1] for i in tstlines]\n",
    "    ytst = [split(i, \":\")[1] for i in ytst]\n",
    "    ytst = [get!(yw2i, w, 1+length(yw2i)) for w in ytst]\n",
    "    xtst = [split(i)[2:end] for i in tstlines]\n",
    "    \n",
    "    # encode sentences and pad or truncate\n",
    "    for (i, line) in enumerate(xtrn)\n",
    "        words = [get!(w2i, w, 1+length(w2i)) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtrn, words)\n",
    "    end\n",
    "    \n",
    "    for (i, line) in enumerate(xtst)\n",
    "        words = [get(w2i, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtst, words)\n",
    "    end\n",
    "    \n",
    "    # concatenate to one big matrix\n",
    "    ixtrn, ixtst = hcat(ixtrn...), hcat(ixtst...)\n",
    "    \n",
    "    # create iterators\n",
    "    dtrn, dtst = minibatch(ixtrn, ytrn, B, shuffle=true), minibatch(ixtst, ytst, B, shuffle=true)\n",
    "    global V = length(w2i) # vocab size\n",
    "    return dtrn, dtst\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([3 13 … 13 13; 4 14 … 37 9450; … ; 2 2 … 2 2; 2 2 … 2 2], [1 2 … 5 2], 50, 5452, false, 5403, 1:5452, true, (40, 5452), (5452,), Array{Int64,2}, Array{Int64,1}), Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([3 13 … 13 13; 2224 2319 … 37 37; … ; 2 2 … 2 2; 2 2 … 2 2], [5 6 … 2 1], 50, 500, false, 451, 1:500, true, (40, 500), (500,), Array{Int64,2}, Array{Int64,1}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn, dtst = readdata(\"./TREC/train_5500.label.txt\", \"./TREC/TREC_10.label.txt\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed layer # may load pretrained embeddings instead\n",
    "struct Embed; w; end\n",
    "Embed(vocabsize::Int, embedsize::Int) = Embed(param(embedsize, vocabsize))\n",
    "(l::Embed)(x) = reshape(l.w[:,x], (size(l.w,1),size(x,1),1,size(x,2)) ) # E,S,Cx,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Conv; w; b; f; p; end\n",
    "function (c::Conv)(x)\n",
    "    conved = conv4(c.w, dropout(x,c.p)) .+ c.b\n",
    "    return c.f.(pool(conved; window=(size(conved, 1), size(conved, 2))))\n",
    "end\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct cnntext\n",
    "    V # vocab size\n",
    "    D # embedding size\n",
    "    C # number of classes\n",
    "    Ci # one\n",
    "    Ks # kernel_sizes\n",
    "    Co # number of each of the sizes\n",
    "    dp # dropout probability\n",
    "    embed::Embed # embedding layer\n",
    "    fc::Dense \n",
    "    conv1::Conv\n",
    "    conv2::Conv\n",
    "    conv3::Conv\n",
    "end\n",
    "\n",
    "function (c::cnntext)(x)\n",
    "    x = c.embed(x)  # E, S, Cx (1), B\n",
    "    x1, x2, x3 = c.conv1(x), c.conv2(x), c.conv3(x) # \n",
    "    x = cat(x1,x2,x3, dims=1)\n",
    "    x = c.fc(x)\n",
    "end\n",
    "\n",
    "(c::cnntext)(x,y) = nll(c(x),y)\n",
    "\n",
    "(c::cnntext)(d::Knet.Data) = mean(c(x,y) for (x,y) in d)\n",
    "\n",
    "function cnntext(V, D, C, Ci, Ks, Co, dp)\n",
    "    cnntext(V, D, C, Ci, Ks, Co, dp, Embed(V,D), Dense(length(Ks)*Co,C,identity,pdrop=dp), Conv(D,Ks[1],Ci,Co), Conv(D,Ks[3],Ci,Co), Conv(D,Ks[3],Ci,Co))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext(9450, 100, 6, 1, [3, 4, 5], 100, 0.5, Embed(P(KnetArray{Float32,2}(100,9450))), Dense(P(KnetArray{Float32,2}(6,300)), P(KnetArray{Float32,1}(6)), identity, 0.5), Conv(P(KnetArray{Float32,4}(100,3,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnntext(V, D, C, Ci, Ks, Co, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(first(dtrn)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takenth(progress(adam(model,ncycle(dtrn,nepochs))),length(dtrn)))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || return\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣███████████████████▉┫ [100.00%, 218/218, 00:15/00:15, 14.67i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.1734474; 0.3351663; 0.037247706; 0.12]\n"
     ]
    }
   ],
   "source": [
    "cnn = trainresults(\"cnntextTREC.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float32,2}:\n",
       " 0.602168  0.173447 \n",
       " 0.677961  0.335166 \n",
       " 0.195046  0.0372477\n",
       " 0.222     0.12     "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const get_word_index = Dict(word=>ii for (ii,word) in enumerate(w2v.vocab))\n",
    "\n",
    "# function get_embedding(word)\n",
    "#     ind = get_word_index[word]\n",
    "#     emb = w2v.embeddings[:,ind]\n",
    "#     return emb\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_embedding(\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency word2vec 300d.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "Pretrained Word2Vec Word emeddings\n",
      "Website: https://code.google.com/archive/p/word2vec/\n",
      "Author: Mikolov et al.\n",
      "Year: 2013\n",
      "\n",
      "Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
      "\n",
      "Paper:\n",
      "    Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/home/saif/.julia/datadeps/word2vec 300d\"?\n",
      "[y/n]\n",
      "stdin> y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.06731994 0.05295623 … -0.21142985 0.013637338; -0.05344657 0.06545979 … -0.0087888 -0.07428761; … ; -0.0073346887 0.010894641 … -0.0040515745 0.015611163; -0.0051456536 -0.047072206 … -0.034157887 0.039655942], [\"</s>\", \"in\", \"for\", \"that\", \"is\", \"on\", \"##\", \"The\", \"with\", \"said\"  …  \"#-###-PA-PARKS\", \"Lackmeyer\", \"PERVEZ\", \"KUNDI\", \"Budhadeb\", \"Nautsch\", \"Antuane\", \"tricorne\", \"VISIONPAD\", \"RAFFAELE\"])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This program has requested access to the data dependency word2vec 300d.\n",
    "# which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
    "\n",
    "# Pretrained Word2Vec Word emeddings\n",
    "# Website: https://code.google.com/archive/p/word2vec/\n",
    "# Author: Mikolov et al.\n",
    "# Year: 2013\n",
    "\n",
    "# Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "# Paper:\n",
    "#     Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "\n",
    "\n",
    "# Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/home/saif/.julia/datadeps/word2vec 300d\"?\n",
    "# [y/n]\n",
    "# stdin> y\n",
    "# const w2v = load_embeddings(Word2Vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
