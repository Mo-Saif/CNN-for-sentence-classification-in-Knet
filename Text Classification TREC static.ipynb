{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Pkg; Pkg.add(\"CorpusLoaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, Random, Plots, IterTools\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten\n",
    "using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.06731994 0.05295623 … -0.21142985 0.013637338; -0.05344657 0.06545979 … -0.0087888 -0.07428761; … ; -0.0073346887 0.010894641 … -0.0040515745 0.015611163; -0.0051456536 -0.047072206 … -0.034157887 0.039655942], [\"</s>\", \"in\", \"for\", \"that\", \"is\", \"on\", \"##\", \"The\", \"with\", \"said\"  …  \"#-###-PA-PARKS\", \"Lackmeyer\", \"PERVEZ\", \"KUNDI\", \"Budhadeb\", \"Nautsch\", \"Antuane\", \"tricorne\", \"VISIONPAD\", \"RAFFAELE\"])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 30 # max sentence length\n",
    "V = length(w2v.vocab) # vocab size\n",
    "B = 50 # batch size\n",
    "D = 100 # embedding size\n",
    "C = 6 # number of classes\n",
    "Ci = 1 # number of input channels\n",
    "Ks = [3,4,5] # kernel_sizes\n",
    "Co = 100 # output channels per kernel size\n",
    "dp = 0.5; # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(w2v.vocab))\n",
    "\n",
    "# function get_embedding(word)\n",
    "#     ind = get_word_index[word]\n",
    "#     emb = w2v.embeddings[:,ind]\n",
    "#     return emb\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw2i = Dict{String,Int}()\n",
    "unk = get_word_index[\"UNK\"]\n",
    "pad = get_word_index[\"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l = length.(xtrn)\n",
    "\n",
    "# using StatsBase;\n",
    "# a = countmap(l);\n",
    "\n",
    "# b = hcat([[key, val] for (key, val) in a]...)';\n",
    "\n",
    "# sort(collect(a), by=x->x[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readdata (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readdata(trnf, tstf, S)\n",
    "    ixtrn = []\n",
    "    ixtst = []\n",
    "    \n",
    "    # read lines\n",
    "    trnlines = readlines(trnf)\n",
    "    \n",
    "    # split label from sentence \n",
    "    ytrn = [split(i)[1] for i in trnlines]\n",
    "    ytrn = [split(i, \":\")[1] for i in ytrn]\n",
    "    xtrn = [split(i)[2:end] for i in trnlines]\n",
    "    \n",
    "    # encode label\n",
    "    ytrn = [get!(yw2i, w, 1+length(yw2i)) for w in ytrn]\n",
    "    \n",
    "    \n",
    "    tstlines = readlines(tstf)\n",
    "    ytst = [split(i)[1] for i in tstlines]\n",
    "    ytst = [split(i, \":\")[1] for i in ytst]\n",
    "    ytst = [get!(yw2i, w, 1+length(yw2i)) for w in ytst]\n",
    "    xtst = [split(i)[2:end] for i in tstlines]\n",
    "    \n",
    "    # encode sentences and pad or truncate\n",
    "    for (i, line) in enumerate(xtrn)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtrn, words)\n",
    "    end\n",
    "    \n",
    "    for (i, line) in enumerate(xtst)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtst, words)\n",
    "    end\n",
    "    \n",
    "    # concatenate to one big matrix\n",
    "    ixtrn, ixtst = hcat(ixtrn...), hcat(ixtst...)\n",
    "    \n",
    "    # create iterators\n",
    "    dtrn, dtst = minibatch(ixtrn, ytrn, B, shuffle=true), minibatch(ixtst, ytst, B, shuffle=true)\n",
    "    return dtrn, dtst\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([1186 469 … 469 469; 93 2461 … 5 2303; … ; 31875 31875 … 31875 31875; 31875 31875 … 31875 31875], [1 2 … 5 2], 50, 5452, false, 5403, 1:5452, true, (30, 5452), (5452,), Array{Int64,2}, Array{Int64,1}), Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([1186 469 … 469 469; 353 904 … 5 5; … ; 31875 31875 … 31875 31875; 31875 31875 … 31875 31875], [5 6 … 2 1], 50, 500, false, 451, 1:500, true, (30, 500), (500,), Array{Int64,2}, Array{Int64,1}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn, dtst = readdata(\"./TREC/train_5500.label.txt\", \"./TREC/TREC_10.label.txt\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed layer # may load pretrained embeddings instead\n",
    "struct Embed; w; end\n",
    "(l::Embed)(x) = reshape(l.w[:,x], (size(l.w,1),size(x,1),1,size(x,2))) # E,S,Cx,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Conv; w; b; f; p; end\n",
    "function (c::Conv)(x)\n",
    "    conved = conv4(c.w, dropout(x,c.p)) .+ c.b\n",
    "    return c.f.(pool(conved; window=(size(conved, 1), size(conved, 2))))\n",
    "end\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct cnntext\n",
    "    V # vocab size\n",
    "    D # embedding size\n",
    "    C # number of classes\n",
    "    Ci # one\n",
    "    Ks # kernel_sizes\n",
    "    Co # number of each of the sizes\n",
    "    dp # dropout probability\n",
    "    embed::Embed # embedding layer\n",
    "    fc::Dense \n",
    "    conv1::Conv\n",
    "    conv2::Conv\n",
    "    conv3::Conv\n",
    "end\n",
    "\n",
    "function (c::cnntext)(x)\n",
    "    x = c.embed(x)  # E, S, Cx (1), B\n",
    "    x1, x2, x3 = c.conv1(x), c.conv2(x), c.conv3(x) # \n",
    "    x = cat(x1,x2,x3, dims=1)\n",
    "    x = c.fc(x)\n",
    "end\n",
    "\n",
    "(c::cnntext)(x,y) = nll(c(x),y)\n",
    "\n",
    "(c::cnntext)(d::Knet.Data) = mean(c(x,y) for (x,y) in d)\n",
    "\n",
    "function cnntext(V, D, C, Ci, Ks, Co, dp)  ##############\n",
    "    cnntext(V, D, C, Ci, Ks, Co, dp, Embed(KnetArray(w2v.embeddings)), Dense(length(Ks)*Co,C,identity,pdrop=dp), Conv(D,Ks[1],Ci,Co), Conv(D,Ks[3],Ci,Co), Conv(D,Ks[3],Ci,Co))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext(929022, 100, 6, 1, [3, 4, 5], 100, 0.5, Embed(K32(300,929022)[0.06731994⋯]), Dense(P(KnetArray{Float32,2}(6,300)), P(KnetArray{Float32,1}(6)), identity, 0.5), Conv(P(KnetArray{Float32,4}(100,3,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnntext(V, D, C, Ci, Ks, Co, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×50 Array{Int64,2}:\n",
       "  1866   1186     469    305   3592  …    2735   1195    469   1186   1186\n",
       "    20    132    3214     11     51          5  20376   1739    132    132\n",
       "    86  41315    7602     12     21      19160     11    933     69   2837\n",
       "   132   1141     263    390    751     279493  75252   4128      2    697\n",
       " 27678  75252      12  75252  75252      75252   2226    172   8354     20\n",
       "   985    933    7402  76413  22695  …   31875  75252  18135  75252     73\n",
       " 75252    184   75252   4810      6      31875     84   2094  31875      2\n",
       " 31875  75252  189872  12936     12      31875    199  75252  31875     12\n",
       " 31875   4572   75252  75252   1476      31875  75252   6543  31875     80\n",
       " 31875  75252   31875  31875   3404      31875  31875  16955  31875  75252\n",
       " 31875    393   31875  31875      3  …   31875  31875      6  31875  31875\n",
       " 31875  11641   31875  31875    127      31875  31875  75252  31875  31875\n",
       " 31875  75252   31875  31875   1207      31875  31875  34068  31875  31875\n",
       "     ⋮                               ⋱       ⋮                            \n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875  …   31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875  …   31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875\n",
       " 31875  31875   31875  31875  31875      31875  31875  31875  31875  31875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(dtrn)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×50 KnetArray{Float32,2}:\n",
       " -0.0213892   -0.024592    -0.0224811   …  -0.0250941   -0.0294992 \n",
       " -0.0699811   -0.0681097   -0.0691206      -0.0680566   -0.0708236 \n",
       "  0.0112531    0.0101576    0.0159278       0.0149379    0.00856043\n",
       " -0.00319843  -0.00451462  -0.00596846     -0.00662423  -0.00621198\n",
       "  0.00986023   0.00986065   0.00481109      0.00233057   0.00811242\n",
       "  0.0760324    0.0596563    0.0725875   …   0.0703006    0.0674989 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(first(dtrn)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        r = ((model(dtrn), model(dtst), accuracy(model,dtrn), accuracy(model,dtst))\n",
    "             for x in takenth(progress(adam(model,ncycle(dtrn,nepochs))),length(dtrn)))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || return\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣███████████████████▉┫ [100.00%, 218/218, 09:41/09:41, 2.66s/i] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.9396954; 0.96873415; 0.5623853; 0.502]\n"
     ]
    }
   ],
   "source": [
    "cnn = trainresults(\"cnntextTREC.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Float32,2}:\n",
       " 1.30894   0.939695\n",
       " 1.31543   0.968734\n",
       " 0.562385  0.724037\n",
       " 0.502     0.754   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This program has requested access to the data dependency word2vec 300d.\n",
    "# which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
    "\n",
    "# Pretrained Word2Vec Word emeddings\n",
    "# Website: https://code.google.com/archive/p/word2vec/\n",
    "# Author: Mikolov et al.\n",
    "# Year: 2013\n",
    "\n",
    "# Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "# Paper:\n",
    "#     Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "\n",
    "\n",
    "# Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/home/saif/.julia/datadeps/word2vec 300d\"?\n",
    "# [y/n]\n",
    "# stdin> y\n",
    "# const w2v = load_embeddings(Word2Vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
