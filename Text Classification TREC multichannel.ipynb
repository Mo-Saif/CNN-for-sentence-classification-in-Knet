{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Pkg; Pkg.add(\"CorpusLoaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, Random, Plots, IterTools\n",
    "using Statistics: mean\n",
    "using Base.Iterators: flatten\n",
    "using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings.EmbeddingTable{Array{Float32,2},Array{String,1}}(Float32[0.06731994 0.05295623 … -0.21142985 0.013637338; -0.05344657 0.06545979 … -0.0087888 -0.07428761; … ; -0.0073346887 0.010894641 … -0.0040515745 0.015611163; -0.0051456536 -0.047072206 … -0.034157887 0.039655942], [\"</s>\", \"in\", \"for\", \"that\", \"is\", \"on\", \"##\", \"The\", \"with\", \"said\"  …  \"#-###-PA-PARKS\", \"Lackmeyer\", \"PERVEZ\", \"KUNDI\", \"Budhadeb\", \"Nautsch\", \"Antuane\", \"tricorne\", \"VISIONPAD\", \"RAFFAELE\"])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 30 # max sentence length\n",
    "# V = length(w2v.vocab) # vocab size\n",
    "B = 50 # batch size\n",
    "D = 100 # embedding size\n",
    "C = 6 # number of classes\n",
    "Ci = 1 # number of input channels\n",
    "Ks = [3,4,5] # kernel_sizes\n",
    "Co = 100 # output channels per kernel size\n",
    "dp = 0.5; # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(w2v.vocab));\n",
    "\n",
    "# function get_embedding(word)\n",
    "#     ind = get_word_index[word]\n",
    "#     emb = w2v.embeddings[:,ind]\n",
    "#     return emb\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yw2i = Dict{String,Int}()\n",
    "unk = get_word_index[\"UNK\"]\n",
    "pad = get_word_index[\"PAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readdata (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function readdata(trnf, tstf, S)\n",
    "    ixtrn = []\n",
    "    ixtst = []\n",
    "    \n",
    "    # read lines\n",
    "    trnlines = readlines(trnf)\n",
    "    \n",
    "    # split label from sentence \n",
    "    ytrn = [split(i)[1] for i in trnlines]\n",
    "    ytrn = [split(i, \":\")[1] for i in ytrn]\n",
    "    xtrn = [split(i)[2:end] for i in trnlines]\n",
    "    \n",
    "    # encode label\n",
    "    ytrn = [get!(yw2i, w, 1+length(yw2i)) for w in ytrn]\n",
    "    \n",
    "    \n",
    "    tstlines = readlines(tstf)\n",
    "    ytst = [split(i)[1] for i in tstlines]\n",
    "    ytst = [split(i, \":\")[1] for i in ytst]\n",
    "    ytst = [get(yw2i, w, 0) for w in ytst]\n",
    "    xtst = [split(i)[2:end] for i in tstlines]\n",
    "    \n",
    "    # encode sentences and pad or truncate\n",
    "    for (i, line) in enumerate(xtrn)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtrn, words)\n",
    "    end\n",
    "    \n",
    "    for (i, line) in enumerate(xtst)\n",
    "        words = [get(get_word_index, w, unk) for w in line]\n",
    "        # pad or truncate\n",
    "        if length(words) >= S\n",
    "            words = words[1:S]\n",
    "        else\n",
    "            words = [words; repeat([pad], S-length(words))]\n",
    "        end\n",
    "        push!(ixtst, words)\n",
    "    end\n",
    "    \n",
    "    # concatenate to one big matrix\n",
    "    ixtrn, ixtst = hcat(ixtrn...), hcat(ixtst...)\n",
    "#     @show size(ixtst)\n",
    "    \n",
    "    # reduce the size of the \n",
    "    unq = [] # unique words in the training and test sets\n",
    "    push!(unq, unique(ixtrn)...)\n",
    "    push!(unq, unique(ixtst)...)\n",
    "    unq = unique(unq)\n",
    "    sort!(unq)\n",
    "#     @show size(unq)\n",
    "    trans = Dict(j => i for (i,j) in enumerate(unq)) \n",
    "    ixtrn = [get(trans, i, 0) for i in ixtrn]\n",
    "#     @show size(ixtrn)\n",
    "    ixtst = [get(trans, i, 0) for i in ixtst]\n",
    "    embeddings = KnetArray(w2v.embeddings[:,unq])\n",
    "    global V = size(embeddings, 2)\n",
    "    # create iterators\n",
    "    dtrn, dtst = minibatch(ixtrn, ytrn, B, shuffle=true), minibatch(ixtst, ytst, B, shuffle=true)\n",
    "    return dtrn, dtst, embeddings\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([988 423 … 423 423; 88 1788 … 4 1699; … ; 6647 6647 … 6647 6647; 6647 6647 … 6647 6647], [1 2 … 5 2], 50, 5452, false, 5403, 1:5452, true, (30, 5452), (5452,), Array{Int64,2}, Array{Int64,1}), Knet.Data{Tuple{Array{Int64,2},Array{Int64,1}}}([988 423 … 423 423; 313 787 … 4 4; … ; 6647 6647 … 6647 6647; 6647 6647 … 6647 6647], [5 6 … 2 1], 50, 500, false, 451, 1:500, true, (30, 500), (500,), Array{Int64,2}, Array{Int64,1}), K32(300,8970)[0.05295623⋯])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn, dtst, embedding = readdata(\"./TREC/train_5500.label.txt\", \"./TREC/TREC_10.label.txt\", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed layer \n",
    "struct Embed; w; w2; end\n",
    "function (l::Embed)(x)\n",
    "    x1 = reshape(l.w[:,x], (size(l.w,1),size(x,1),1,size(x,2))) # E,S,Cx,B\n",
    "    x2 = reshape(l.w2[:,x], (size(l.w2,1),size(x,1),1,size(x,2))) # E,S,Cx,B\n",
    "    return x1, x2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Dense; w; b; f; p; end\n",
    "(d::Dense)(x) = d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Conv; w; b; f; p; end\n",
    "function (c::Conv)(x)\n",
    "    conved = conv4(c.w, dropout(x,c.p)) .+ c.b\n",
    "    return c.f.(pool(conved; window=(size(conved, 1), size(conved, 2))))\n",
    "end\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct cnntext\n",
    "    V # vocab size\n",
    "    D # embedding size\n",
    "    C # number of classes\n",
    "    Ci # one\n",
    "    Ks # kernel_sizes\n",
    "    Co # number of each of the sizes\n",
    "    dp # dropout probability\n",
    "    embed::Embed # embedding layer\n",
    "    fc::Dense \n",
    "    conv1::Conv\n",
    "    conv2::Conv\n",
    "    conv3::Conv\n",
    "end\n",
    "\n",
    "function (c::cnntext)(x)\n",
    "    x, xc = c.embed(x)  # E, S, Cx (1), B\n",
    "    x1, x2, x3 = c.conv1(x), c.conv2(x), c.conv3(x) \n",
    "    x4, x5, x6 = c.conv1(xc), c.conv2(xc), c.conv3(xc)\n",
    "    x1 = x1 + x4\n",
    "    x2 = x2 + x5\n",
    "    x3 = x3 + x6\n",
    "    x = cat(x1,x2,x3, dims=1)\n",
    "    x = c.fc(x)\n",
    "end\n",
    "\n",
    "(c::cnntext)(x,y) = nll(c(x),y)\n",
    "\n",
    "(c::cnntext)(d::Knet.Data) = mean(c(x,y) for (x,y) in d)\n",
    "\n",
    "function cnntext(V, D, C, Ci, Ks, Co, dp)  ##############\n",
    "    cnntext(V, D, C, Ci, Ks, Co, dp, Embed(Param(copy(embedding)), embedding), Dense(length(Ks)*Co,C,identity,pdrop=dp), Conv(D,Ks[1],Ci,Co), Conv(D,Ks[3],Ci,Co), Conv(D,Ks[3],Ci,Co))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnntext(8970, 100, 6, 1, [3, 4, 5], 100, 0.5, Embed(P(KnetArray{Float32,2}(300,8970)), K32(300,8970)[0.05295623⋯]), Dense(P(KnetArray{Float32,2}(6,300)), P(KnetArray{Float32,1}(6)), identity, 0.5), Conv(P(KnetArray{Float32,4}(100,3,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0), Conv(P(KnetArray{Float32,4}(100,5,1,100)), P(KnetArray{Float32,4}(1,1,100,1)), NNlib.relu, 0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnntext(V, D, C, Ci, Ks, Co, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×50 Array{Int64,2}:\n",
       " 1935   423   988  1935   423   423  …   423   423   423   423   423   423\n",
       "    4   169  1322  5383  3867    56        4     4     9     4    55     4\n",
       "   10    10   123  7906  7457  7906       10    10    10  7906    40  7906\n",
       "  401   467    10    19   139   108     1238   869  2917   320   281  7628\n",
       " 7906  5284  4951   164  7906    88      160     2  7906  6200  7906  7906\n",
       " 8782   956    15  7906  1466  6338  …   397  3221  5286    57  7906  6647\n",
       " 7906  7906    58  7906  7906  8456     7906  7906  4885  5538  7735  6647\n",
       " 6647  6647   287  4853   670  1921     7906  6647  6198  3075  7906  6647\n",
       " 6647  6647  8477  8298  7906  7906     7906  6647   102  7906     1  6647\n",
       " 6647  6647  8464  7906  6647  6647     3910  6647  7906  6647  5042  6647\n",
       " 6647  6647  8385  4938  6647  6647  …  7906  6647  7906  6647  7906  6647\n",
       " 6647  6647  7906  7906  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647    19  6647  6647     6647  6647  6647  6647  6647  6647\n",
       "    ⋮                             ⋮  ⋱           ⋮                        \n",
       " 6647  6647  6647  4169  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6964  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  2334  6647  6647  …  6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  7906  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647  …  6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647\n",
       " 6647  6647  6647  6647  6647  6647     6647  6647  6647  6647  6647  6647"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(dtrn)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×50 KnetArray{Float32,2}:\n",
       "  0.0477929   0.0320577   0.0359198  …   0.0479496   0.0546228   0.0420403\n",
       "  0.0332731   0.0322033   0.0251398      0.0390904   0.0385036   0.0352652\n",
       " -0.0889302  -0.0835455  -0.0874306     -0.0769347  -0.0957419  -0.0864299\n",
       " -0.0406678  -0.037506   -0.0393853     -0.0336487  -0.0266294  -0.0444744\n",
       " -0.035897   -0.0398759  -0.0347919     -0.0291215  -0.0336546  -0.0322867\n",
       "  0.0492987   0.0432989   0.0459299  …   0.0430649   0.0657053   0.0451848"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(first(dtrn)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        r = ((model(dtrn), model(dtst), accuracy(model,dtrn), accuracy(model,dtst))\n",
    "             for x in takenth(progress(adam(model,ncycle(dtrn,nepochs))),length(dtrn)))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || return\n",
    "        r = Knet.load(file,\"results\")\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣███████████████████▉┫ [100.00%, 327/327, 27:28/27:28, 5.04s/i] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.2219617; 0.33521825; 0.7293578; 0.742]\n"
     ]
    }
   ],
   "source": [
    "cnn = trainresults(\"cnntextTREC.jld2\", model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float32,2}:\n",
       " 0.851961  0.434623  0.221962\n",
       " 0.931804  0.503326  0.335218\n",
       " 0.729358  0.863486  0.95156 \n",
       " 0.742     0.806     0.894   "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This program has requested access to the data dependency word2vec 300d.\n",
    "# which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
    "\n",
    "# Pretrained Word2Vec Word emeddings\n",
    "# Website: https://code.google.com/archive/p/word2vec/\n",
    "# Author: Mikolov et al.\n",
    "# Year: 2013\n",
    "\n",
    "# Pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "\n",
    "# Paper:\n",
    "#     Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "\n",
    "\n",
    "\n",
    "# Do you want to download the dataset from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz to \"/home/saif/.julia/datadeps/word2vec 300d\"?\n",
    "# [y/n]\n",
    "# stdin> y\n",
    "# const w2v = load_embeddings(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l = length.(xtrn)\n",
    "\n",
    "# using StatsBase;\n",
    "# a = countmap(l);\n",
    "\n",
    "# b = hcat([[key, val] for (key, val) in a]...)';\n",
    "\n",
    "# sort(collect(a), by=x->x[2]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
